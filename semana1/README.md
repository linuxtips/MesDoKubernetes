# MesDoKubernetes

### O que √© o Kubernetes?

**Vers√£o resumida:**

O projeto Kubernetes foi desenvolvido pela Google, em meados de 2014, para atuar como um orquestrador de cont√™ineres para a empresa. O Kubernetes (k8s), cujo termo em Grego significa "timoneiro", √© um projeto *open source* que conta com *design* e desenvolvimento baseados no projeto Borg, que tamb√©m √© da Google [1](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/). Alguns outros produtos dispon√≠veis no mercado, tais como o Apache Mesos e o Cloud Foundry, tamb√©m surgiram a partir do projeto Borg.

Como Kubernetes √© uma palavra dif√≠cil de se pronunciar - e de se escrever - a comunidade simplesmente o apelidou de **k8s**, seguindo o padr√£o [i18n](http://www.i18nguy.com/origini18n.html) (a letra "k" seguida por oito letras e o "s" no final), pronunciando-se simplesmente "kates".

**Vers√£o longa:**

Praticamente todo software desenvolvido na Google √© executado em cont√™iner [2](https://www.enterpriseai.news/2014/05/28/google-runs-software-containers/). A Google j√° gerencia cont√™ineres em larga escala h√° mais de uma d√©cada, quando n√£o se falava tanto sobre isso. Para atender a demanda interna, alguns desenvolvedores do Google constru√≠ram tr√™s sistemas diferentes de gerenciamento de cont√™ineres: **Borg**, **Omega** e **Kubernetes**. Cada sistema teve o desenvolvimento bastante influenciado pelo antecessor, embora fosse desenvolvido por diferentes raz√µes.

O primeiro sistema de gerenciamento de cont√™ineres desenvolvido no Google foi o Borg, constru√≠do para gerenciar servi√ßos de longa dura√ß√£o e jobs em lote, que anteriormente eram tratados por dois sistemas:  **Babysitter** e **Global Work Queue**. O √∫ltimo influenciou fortemente a arquitetura do Borg, mas estava focado em execu√ß√£o de jobs em lote. O Borg continua sendo o principal sistema de gerenciamento de cont√™ineres dentro do Google por causa de sua escala, variedade de recursos e robustez extrema.

O segundo sistema foi o Omega, descendente do Borg. Ele foi impulsionado pelo desejo de melhorar a engenharia de software do ecossistema Borg. Esse sistema aplicou muitos dos padr√µes que tiveram sucesso no Borg, mas foi constru√≠do do zero para ter a arquitetura mais consistente. Muitas das inova√ß√µes do Omega foram posteriormente incorporadas ao Borg.

O terceiro sistema foi o Kubernetes. Concebido e desenvolvido em um mundo onde desenvolvedores externos estavam se interessando em cont√™ineres e o Google desenvolveu um neg√≥cio em amplo crescimento atualmente, que √© a venda de infraestrutura de nuvem p√∫blica.

O Kubernetes √© de c√≥digo aberto - em contraste com o Borg e o Omega que foram desenvolvidos como sistemas puramente internos do Google. O Kubernetes foi desenvolvido com um foco mais forte na experi√™ncia de desenvolvedores que escrevem aplicativos que s√£o executados em um cluster: seu principal objetivo √© facilitar a implanta√ß√£o e o gerenciamento de sistemas distribu√≠dos, enquanto se beneficia do melhor uso de recursos de mem√≥ria e processamento que os cont√™ineres possibilitam.

Estas informa√ß√µes foram extra√≠das e adaptadas deste [artigo](https://static.googleusercontent.com/media/research.google.com/pt-BR//pubs/archive/44843.pdf), que descreve as li√ß√µes aprendidas com o desenvolvimento e opera√ß√£o desses sistemas.
&nbsp;
### Arquitetura do k8s

Assim como os demais orquestradores dispon√≠veis, o k8s tamb√©m segue um modelo *control plane/workers*, constituindo assim um *cluster*, onde para seu funcionamento √© recomendado no m√≠nimo tr√™s n√≥s: o n√≥ *control-plane*, respons√°vel (por padr√£o) pelo gerenciamento do *cluster*, e os demais como *workers*, executores das aplica√ß√µes que queremos executar sobre esse *cluster*.

√â poss√≠vel criar um cluster Kubernetes rodando em apenas um n√≥, por√©m √© recomendado somente para fins de estudos e nunca executado em ambiente produtivo.

Caso voc√™ queira utilizar o Kubernetes em sua m√°quina local, em seu desktop, existem diversas solu√ß√µes que ir√£o criar um cluster Kubernetes, utilizando m√°quinas virtuais ou o Docker, por exemplo.

Com isso voc√™ poder√° ter um cluster Kubernetes com diversos n√≥s, por√©m todos eles rodando em sua m√°quina local, em seu desktop.

Alguns exemplos s√£o:

* [Kind](https://kind.sigs.k8s.io/docs/user/quick-start): Uma ferramenta para execu√ß√£o de cont√™ineres Docker que simulam o funcionamento de um cluster Kubernetes. √â utilizado para fins did√°ticos, de desenvolvimento e testes. O **Kind n√£o deve ser utilizado para produ√ß√£o**;

* [Minikube](https://github.com/kubernetes/minikube): ferramenta para implementar um *cluster* Kubernetes localmente com apenas um n√≥. Muito utilizado para fins did√°ticos, de desenvolvimento e testes. O **Minikube n√£o deve ser utilizado para produ√ß√£o**;

* [MicroK8S](https://microk8s.io): Desenvolvido pela [Canonical](https://canonical.com), mesma empresa que desenvolve o [Ubuntu](https://ubuntu.com). Pode ser utilizado em diversas distribui√ß√µes e **pode ser utilizado em ambientes de produ√ß√£o**, em especial para *Edge Computing* e IoT (*Internet of things*);

* [k3s](https://k3s.io): Desenvolvido pela [Rancher Labs](https://rancher.com), √© um concorrente direto do MicroK8s, podendo ser executado inclusive em Raspberry Pi;

* [k0s](https://k0sproject.io): Desenvolvido pela [Mirantis](https://www.mirantis.com), mesma empresa que adquiriu a parte enterprise do [Docker](https://www.docker.com). √â uma distribui√ß√£o do Kubernetes com todos os recursos necess√°rios para funcionar em um √∫nico bin√°rio, que proporciona uma simplicidade na instala√ß√£o e manuten√ß√£o do cluster. A pron√∫ncia √© correta √© kay-zero-ess e tem por objetivo reduzir o esfor√ßo t√©cnico e desgaste na instala√ß√£o de um cluster Kubernetes, por isso o seu nome faz alus√£o a *Zero Friction*. **O k0s pode ser utilizado em ambientes de produ√ß√£o**;

* **API Server**: √â um dos principais componentes do k8s. Este componente fornece uma API que utiliza JSON sobre HTTP para comunica√ß√£o, onde para isto √© utilizado principalmente o utilit√°rio ``kubectl``, por parte dos administradores, para a comunica√ß√£o com os demais n√≥s, como mostrado no gr√°fico. Estas comunica√ß√µes entre componentes s√£o estabelecidas atrav√©s de requisi√ß√µes [REST](https://restfulapi.net);

* **etcd**: O etcd √© um *datastore* chave-valor distribu√≠do que o k8s utiliza para armazenar as especifica√ß√µes, status e configura√ß√µes do *cluster*. Todos os dados armazenados dentro do etcd s√£o manipulados apenas atrav√©s da API. Por quest√µes de seguran√ßa, o etcd √© por padr√£o executado apenas em n√≥s classificados como *control plane* no *cluster* k8s, mas tamb√©m podem ser executados em *clusters* externos, espec√≠ficos para o etcd, por exemplo;

* **Scheduler**: O *scheduler* √© respons√°vel por selecionar o n√≥ que ir√° hospedar um determinado *pod* (a menor unidade de um *cluster* k8s - n√£o se preocupe sobre isso por enquanto, n√≥s falaremos mais sobre isso mais tarde) para ser executado. Esta sele√ß√£o √© feita baseando-se na quantidade de recursos dispon√≠veis em cada n√≥, como tamb√©m no estado de cada um dos n√≥s do *cluster*, garantindo assim que os recursos sejam bem distribu√≠dos. Al√©m disso, a sele√ß√£o dos n√≥s, na qual um ou mais pods ser√£o executados, tamb√©m pode levar em considera√ß√£o pol√≠ticas definidas pelo usu√°rio, tais como afinidade, localiza√ß√£o dos dados a serem lidos pelas aplica√ß√µes, etc;

* **Controller Manager**: √â o *controller manager* quem garante que o *cluster* esteja no √∫ltimo estado definido no etcd. Por exemplo: se no etcd um *deploy* est√° configurado para possuir dez r√©plicas de um *pod*, √© o *controller manager* quem ir√° verificar se o estado atual do *cluster* corresponde a este estado e, em caso negativo, procurar√° conciliar ambos;

* **Kubelet**: O *kubelet* pode ser visto como o agente do k8s que √© executado nos n√≥s workers. Em cada n√≥ worker dever√° existir um agente Kubelet em execu√ß√£o. O Kubelet √© respons√°vel por de fato gerenciar os *pods* que foram direcionados pelo *controller* do *cluster*, dentro dos n√≥s, de forma que para isto o Kubelet pode iniciar, parar e manter os cont√™ineres e os pods em funcionamento de acordo com o instru√≠do pelo controlador do cluster;

* **Kube-proxy**: Age como um *proxy* e um *load balancer*. Este componente √© respons√°vel por efetuar roteamento de requisi√ß√µes para os *pods* corretos, como tamb√©m por cuidar da parte de rede do n√≥;

&nbsp;
### Portas que devemos nos preocupar

**CONTROL PLANE**

Protocol|Direction|Port Range|Purpose|Used By
--------|---------|----------|-------|-------
TCP|Inbound|6443*|Kubernetes API server|All
TCP|Inbound|2379-2380|etcd server client API|kube-apiserver, etcd
TCP|Inbound|10250|Kubelet API|Self, Control plane
TCP|Inbound|10251|kube-scheduler|Self
TCP|Inbound|10252|kube-controller-manager|Self

* Toda porta marcada por * √© customiz√°vel, voc√™ precisa se certificar que a porta alterada tamb√©m esteja aberta.


&nbsp;
**WORKERS**

Protocol|Direction|Port Range|Purpose|Used By
--------|---------|----------|-------|-------
TCP|Inbound|10250|Kubelet API|Self, Control plane
TCP|Inbound|30000-32767|NodePort|Services All

&nbsp;
### Conceitos-chave do k8s

√â importante saber que a forma como o k8s gerencia os cont√™ineres √© ligeiramente diferente de outros orquestradores, como o Docker Swarm, sobretudo devido ao fato de que ele n√£o trata os cont√™ineres diretamente, mas sim atrav√©s de *pods*. Vamos conhecer alguns dos principais conceitos que envolvem o k8s a seguir:

- **Pod**: √â o menor objeto do k8s. Como dito anteriormente, o k8s n√£o trabalha com os cont√™ineres diretamente, mas organiza-os dentro de *pods*, que s√£o abstra√ß√µes que dividem os mesmos recursos, como endere√ßos, volumes, ciclos de CPU e mem√≥ria. Um pod pode possuir v√°rios cont√™ineres;

- **Deployment**: √â um dos principais *controllers* utilizados. O *Deployment*, em conjunto com o *ReplicaSet*, garante que determinado n√∫mero de r√©plicas de um pod esteja em execu√ß√£o nos n√≥s workers do cluster. Al√©m disso, o Deployment tamb√©m √© respons√°vel por gerenciar o ciclo de vida das aplica√ß√µes, onde caracter√≠sticas associadas a aplica√ß√£o, tais como imagem, porta, volumes e vari√°veis de ambiente, podem ser especificados em arquivos do tipo *yaml* ou *json* para posteriormente serem passados como par√¢metro para o ``kubectl`` executar o deployment. Esta a√ß√£o pode ser executada tanto para cria√ß√£o quanto para atualiza√ß√£o e remo√ß√£o do deployment;

- **ReplicaSets**: √â um objeto respons√°vel por garantir a quantidade de pods em execu√ß√£o no n√≥;

- **Services**: √â uma forma de voc√™ expor a comunica√ß√£o atrav√©s de um *ClusterIP*, *NodePort* ou *LoadBalancer* para distribuir as requisi√ß√µes entre os diversos Pods daquele Deployment. Funciona como um balanceador de carga.


### Instalando e customizando o Kubectl

#### Instala√ß√£o do Kubectl no GNU/Linux

Vamos instalar o ``kubectl`` com os seguintes comandos.

```
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
```
&nbsp;
#### Instala√ß√£o do Kubectl no MacOS

O ``kubectl`` pode ser instalado no MacOS utilizando tanto o [Homebrew](https://brew.sh), quanto o m√©todo tradicional. Com o Homebrew j√° instalado, o kubectl pode ser instalado da seguinte forma.

```
sudo brew install kubectl

kubectl version --client
```
&nbsp;
Ou:

```
sudo brew install kubectl-cli

kubectl version --client
```
&nbsp;
J√° com o m√©todo tradicional, a instala√ß√£o pode ser realizada com os seguintes comandos.

```
curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl"

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
```
&nbsp;
#### Instala√ß√£o do Kubectl no Windows

A instala√ß√£o do ``kubectl`` pode ser realizada efetuando o download [neste link](https://dl.k8s.io/release/v1.24.3/bin/windows/amd64/kubectl.exe). 

Outras informa√ß√µes sobre como instalar o kubectl no Windows podem ser encontradas [nesta p√°gina](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/).


### Customizando o kubectl

#### Auto-complete

Execute o seguinte comando para configurar o alias e autocomplete para o ``kubectl``.

No Bash:

```bash
source <(kubectl completion bash) # configura o autocomplete na sua sess√£o atual (antes, certifique-se de ter instalado o pacote bash-completion).

echo "source <(kubectl completion bash)" >> ~/.bashrc # add autocomplete permanentemente ao seu shell.
```
&nbsp;
No ZSH:

```bash 
source <(kubectl completion zsh)

echo "[[ $commands[kubectl] ]] && source <(kubectl completion zsh)"
```
&nbsp;
#### Criando um alias para o kubectl

Crie o alias ``k`` para ``kubectl``:

```
alias k=kubectl

complete -F __start_kubectl k
```
&nbsp;
### Criando um cluster Kubernetes

### Criando o cluster em sua m√°quina local

Vamos mostrar algumas op√ß√µes, caso voc√™ queira come√ßar a brincar com o Kubernetes utilizando somente a sua m√°quina local, o seu desktop.

Lembre-se, voc√™ n√£o √© obrigado a testar/utilizar todas as op√ß√µes abaixo, mas seria muito bom caso voc√™ testasse. :D

#### Minikube

##### Requisitos b√°sicos

√â importante frisar que o Minikube deve ser instalado localmente, e n√£o em um *cloud provider*. Por isso, as especifica√ß√µes de *hardware* a seguir s√£o referentes √† m√°quina local.

* Processamento: 1 core;
* Mem√≥ria: 2 GB;
* HD: 20 GB.

##### Instala√ß√£o do Minikube no GNU/Linux

Antes de mais nada, verifique se a sua m√°quina suporta virtualiza√ß√£o. No GNU/Linux, isto pode ser realizado com o seguinte comando:

```
grep -E --color 'vmx|svm' /proc/cpuinfo
```
&nbsp;
Caso a sa√≠da do comando n√£o seja vazia, o resultado √© positivo.

H√° a possibilidade de n√£o utilizar um *hypervisor* para a instala√ß√£o do Minikube, executando-o ao inv√©s disso sobre o pr√≥prio host. Iremos utilizar o Oracle VirtualBox como *hypervisor*, que pode ser encontrado [aqui](https://www.virtualbox.org).

Efetue o download e a instala√ß√£o do ``Minikube`` utilizando os seguintes comandos.

```
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
```
&nbsp;
##### Instala√ß√£o do Minikube no MacOS

No MacOS, o comando para verificar se o processador suporta virtualiza√ß√£o √©:

```
sysctl -a | grep -E --color 'machdep.cpu.features|VMX'
```
&nbsp;
Se voc√™ visualizar `VMX` na sa√≠da, o resultado √© positivo.

Efetue a instala√ß√£o do Minikube com um dos dois m√©todos a seguir, podendo optar-se pelo Homebrew ou pelo m√©todo tradicional.

```
sudo brew install minikube

minikube version
```
&nbsp;
Ou:

```
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
```
&nbsp;
##### Instala√ß√£o do Minikube no Microsoft Windows

No Microsoft Windows, voc√™ deve executar o comando `systeminfo` no prompt de comando ou no terminal. Caso o retorno deste comando seja semelhante com o descrito a seguir, ent√£o a virtualiza√ß√£o √© suportada.

```
Hyper-V Requirements:     VM Monitor Mode Extensions: Yes
                          Virtualization Enabled In Firmware: Yes
                          Second Level Address Translation: Yes
                          Data Execution Prevention Available: Yes
```
&nbsp;
Caso a linha a seguir tamb√©m esteja presente, n√£o √© necess√°ria a instala√ß√£o de um *hypervisor* como o Oracle VirtualBox:

```
Hyper-V Requirements:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.
```
&nbsp;
Fa√ßa o download e a instala√ß√£o de um *hypervisor* (preferencialmente o [Oracle VirtualBox](https://www.virtualbox.org)), caso no passo anterior n√£o tenha sido acusada a presen√ßa de um. Finalmente, efetue o download do instalador do Minikube [aqui](https://github.com/kubernetes/minikube/releases/latest) e execute-o.


##### Iniciando, parando e excluindo o Minikube

Quando operando em conjunto com um *hypervisor*, o Minikube cria uma m√°quina virtual, onde dentro dela estar√£o todos os componentes do k8s para execu√ß√£o.

√â poss√≠vel selecionar qual *hypervisor* iremos utilizar por padr√£o, atrav√©s no comando abaixo:

```
minikube config set driver <SEU_HYPERVISOR> 
```
&nbsp;
Voc√™ deve substituir <SEU_HYPERVISOR> pelo seu hypervisor, por exemplo o KVM2, QEMU, Virtualbox ou o Hyperkit.


Caso n√£o queria configurar um hypervisor padr√£o, voc√™ pode digitar o comando ``minikube start --driver=hyperkit`` toda vez que criar um novo ambiente. 


##### Certo, e como eu sei que est√° tudo funcionando como deveria?

Uma vez iniciado, voc√™ deve ter uma sa√≠da na tela similar √† seguinte:

```
minikube start

üòÑ  minikube v1.26.0 on Debian bookworm/sid
‚ú®  Using the qemu2 (experimental) driver based on user configuration
üëç  Starting control plane node minikube in cluster minikube
üî•  Creating qemu2 VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.16 ...
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: default-storageclass, storage-provisioner
üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

```

Voc√™ pode ent√£o listar os n√≥s que fazem parte do seu *cluster* k8s com o seguinte comando:

```
kubectl get nodes
```
&nbsp;
A sa√≠da ser√° similar ao conte√∫do a seguir:

```
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   20s   v1.25.3
```
&nbsp;
Para criar um cluster com mais de um n√≥, voc√™ pode utilizar o comando abaixo, apenas modificando os valores para o desejado:

```
minikube start --nodes 2 -p multinode-cluster

üòÑ  minikube v1.26.0 on Debian bookworm/sid
‚ú®  Automatically selected the docker driver. Other choices: kvm2, virtualbox, ssh, none, qemu2 (experimental)
üìå  Using Docker driver with root privileges
üëç  Starting control plane node minikube in cluster minikube
üöú  Pulling base image ...
üíæ  Downloading Kubernetes v1.24.1 preload ...
    > preloaded-images-k8s-v18-v1...: 405.83 MiB / 405.83 MiB  100.00% 66.78 Mi
    > gcr.io/k8s-minikube/kicbase: 385.99 MiB / 386.00 MiB  100.00% 23.63 MiB p
    > gcr.io/k8s-minikube/kicbase: 0 B [_________________________] ?% ? p/s 11s
üî•  Creating docker container (CPUs=2, Memory=8000MB) ...
üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîó  Configuring CNI (Container Networking Interface) ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass

üëç  Starting worker node minikube-m02 in cluster minikube
üöú  Pulling base image ...
üî•  Creating docker container (CPUs=2, Memory=8000MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=192.168.11.11
üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    ‚ñ™ env NO_PROXY=192.168.11.11
üîé  Verifying Kubernetes components...
üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

```
&nbsp;
Para visualizar os n√≥s do seu novo cluster Kubernetes, digite:

```
kubectl get nodes
```
&nbsp;
Inicialmente, a inten√ß√£o do Minikube √© executar o k8s em apenas um n√≥, por√©m a partir da vers√£o 1.10.1 √© poss√≠vel usar a fun√ß√£o de multi-node.

Caso os comandos anteriores tenham sido executados sem erro, a instala√ß√£o do Minikube ter√° sido realizada com sucesso.

##### Ver detalhes sobre o cluster 

```
minikube status
```
&nbsp;
##### Descobrindo o endere√ßo do Minikube

Como dito anteriormente, o Minikube ir√° criar uma m√°quina virtual, assim como o ambiente para a execu√ß√£o do k8s localmente. Ele tamb√©m ir√° configurar o ``kubectl`` para comunicar-se com o Minikube. Para saber qual √© o endere√ßo IP dessa m√°quina virtual, pode-se executar:

```
minikube ip
```
&nbsp;
O endere√ßo apresentado √© que deve ser utilizado para comunica√ß√£o com o k8s.

##### Acessando a m√°quina do Minikube via SSH

Para acessar a m√°quina virtual criada pelo Minikube, pode-se executar:

```
minikube ssh
```
&nbsp;
##### Dashboard do Minikube

O Minikube vem com um *dashboard* *web* interessante para que o usu√°rio iniciante observe como funcionam os *workloads* sobre o k8s. Para habilit√°-lo, o usu√°rio pode digitar:

```
minikube dashboard
```
&nbsp;
##### Logs do Minikube

Os *logs* do Minikube podem ser acessados atrav√©s do seguinte comando.

```
minikube logs
```
&nbsp;
##### Remover o cluster 

```
minikube delete
```
&nbsp;
Caso queira remover o cluster e todos os arquivos referente a ele, utilize o parametro *--purge*, conforme abaixo:

```
minikube delete --purge
```
&nbsp;
#### Kind

O Kind (*Kubernetes in Docker*) √© outra alternativa para executar o Kubernetes num ambiente local para testes e aprendizado, mas n√£o √© recomendado para uso em produ√ß√£o.

##### Instala√ß√£o no GNU/Linux

Para fazer a instala√ß√£o no GNU/Linux, execute os seguintes comandos.

```
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64

chmod +x ./kind

sudo mv ./kind /usr/local/bin/kind
```
&nbsp;
##### Instala√ß√£o no MacOS

Para fazer a instala√ß√£o no MacOS, execute o seguinte comando.

```
sudo brew install kind
```
&nbsp;
ou

```
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-darwin-amd64
chmod +x ./kind
mv ./kind /usr/bin/kind
```
&nbsp;
##### Instala√ß√£o no Windows

Para fazer a instala√ß√£o no Windows, execute os seguintes comandos.

```
curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64

Move-Item .\kind-windows-amd64.exe c:\kind.exe
```
&nbsp;
###### Instala√ß√£o no Windows via Chocolatey

Execute o seguinte comando para instalar o Kind no Windows usando o Chocolatey.

```
choco install kind
```
&nbsp;
##### Criando um cluster com o Kind

Ap√≥s realizar a instala√ß√£o do Kind, vamos iniciar o nosso cluster.

```
kind create cluster

Creating cluster "kind" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Not sure what to do next? üòÖ  Check out https://kind.sigs.k8s.io/docs/user/quick-start/

```
&nbsp;
√â poss√≠vel criar mais de um cluster e personalizar o seu nome.

```
kind create cluster --name giropops

Creating cluster "giropops" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
Set kubectl context to "kind-giropops"
You can now use your cluster with:

kubectl cluster-info --context kind-giropops

Thanks for using kind! üòä
```
&nbsp;
Para visualizar os seus clusters utilizando o kind, execute o comando a seguir.

```
kind get clusters
```
&nbsp;
Liste os nodes do cluster.

```
kubectl get nodes
```
&nbsp;
##### Criando um cluster com m√∫ltiplos n√≥s locais com o Kind

√â poss√≠vel para essa aula incluir m√∫ltiplos n√≥s na estrutura do Kind, que foi mencionado anteriormente.

Execute o comando a seguir para selecionar e remover todos os clusters locais criados no Kind.

```
kind delete clusters $(kind get clusters)

Deleted clusters: ["giropops" "kind"]
```
&nbsp;
Crie um arquivo de configura√ß√£o para definir quantos e o tipo de n√≥s no cluster que voc√™ deseja. No exemplo a seguir, ser√° criado o arquivo de configura√ß√£o ``kind-3nodes.yaml`` para especificar um cluster com 1 n√≥ control-plane (que executar√° o control plane) e 2 workers.

```
cat << EOF > $HOME/kind-3nodes.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF
```
&nbsp;
Agora vamos criar um cluster chamado ``kind-multinodes`` utilizando as especifica√ß√µes definidas no arquivo ``kind-3nodes.yaml``.

```
kind create cluster --name kind-multinodes --config $HOME/kind-3nodes.yaml

Creating cluster "kind-multinodes" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶ üì¶ üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
 ‚úì Joining worker nodes üöú 
Set kubectl context to "kind-kind-multinodes"
You can now use your cluster with:

kubectl cluster-info --context kind-kind-multinodes

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ
```
&nbsp;
Valide a cria√ß√£o do cluster com o comando a seguir.

```
kubectl get nodes
```
&nbsp;
Mais informa√ß√µes sobre o Kind est√£o dispon√≠veis em: https://kind.sigs.k8s.io

&nbsp;
### Primeiros passos no k8s
&nbsp;

### Instala√ß√£o de um cluster Kubernetes


#### O que √© um cluster Kubernetes?

Um cluster Kubernetes √© um conjunto de nodes que trabalham juntos para executar todos os nossos `pods`. Um cluster Kubernetes √© composto por nodes que podem ser tanto `control plane` quanto `workers`. O `control plane` √© respons√°vel por gerenciar o cluster, enquanto os `workers` s√£o respons√°veis por executar os `pods` que s√£o criados no cluster pelos usu√°rios.

Quando estamos pensando em um cluster Kubernetes, precisamos lembrar que a principal fun√ß√£o do Kubernetes √© orquestrar containers. O Kubernetes √© um orquestrador de containers, sendo assim quando estamos falando de um cluster Kubernetes, estamos falando de um cluster de orquestradores de containers. Eu sempre gosto de pensar em um cluster Kubernetes como se fosse uma orquestra, onde temos uma pessoa regendo a orquestra, que √© o `control plane`, e temos as pessoas musicistas, que est√£o executando os instrumentos, que s√£o os `workers`.

Sendo assim, o `control plane` √© respons√°vel por gerenciar o cluster, como por exemplo:

* Criar e gerenciar os recursos do cluster, como por exemplo, `namespaces`, `deployments`, `services`, `configmaps`, `secrets`, etc.
* Gerenciar os `workers` do cluster.
* Gerenciar a rede do cluster. 
* O `etcd` desempenha um papel crucial na manuten√ß√£o da estabilidade e confiabilidade do cluster. Ele armazena as informa√ß√µes de configura√ß√£o de todos os componentes do `control plane`, incluindo os detalhes dos servi√ßos, `pods` e outros recursos do cluster. Gra√ßas ao seu design distribu√≠do, o `etcd` √© capaz de tolerar falhas e garantir a continuidade dos dados, mesmo em caso de falha de um ou mais n√≥s. Al√©m disso, ele suporta a comunica√ß√£o segura entre os componentes do cluster, usando criptografia `TLS` para proteger os dados.

* O `scheduler` √© o componente respons√°vel por decidir em qual n√≥ os `pods` ser√£o executados, levando em considera√ß√£o os requisitos e os recursos dispon√≠veis. O `scheduler` tamb√©m monitora constantemente a situa√ß√£o do cluster e, se necess√°rio, ajusta a distribui√ß√£o dos pods para garantir a melhor utiliza√ß√£o dos recursos e manter a harmonia entre os componentes. 

* O `controller-manager` √© respons√°vel por gerenciar os diferentes controladores que regulam o estado do cluster e mant√™m tudo funcionando. Ele monitora constantemente o estado atual dos recursos e compara-os com o estado desejado, fazendo ajustes conforme necess√°rio.

* Onde est√° o `api-server` √© o componente central que exp√µe a API do Kubernetes, permitindo que outros componentes do `control plane`, como o `controller-manager` e o `scheduler`, bem como ferramentas externas, se comuniquem e interajam com o cluster. O `api-server` √© a principal interface de comunica√ß√£o do Kubernetes, autenticando e autorizando solicita√ß√µes, processando-as e fornecendo as respostas apropriadas. Ele garante que as informa√ß√µes sejam compartilhadas e acessadas de forma segura e eficiente, possibilitando uma colabora√ß√£o harmoniosa entre todos os componentes do cluster.


J√° no lado dos `workers`, as coisa s√£o bem mais simples, pois a principal fun√ß√£o deles √© executar os `pods` que s√£o criados no cluster pelos usu√°rios. Nos `workers` n√≥s temos, por padr√£o, os seguintes componentes do Kubernetes:

* O `kubelet` √© o agente que funciona em cada n√≥ do cluster, garantindo que os containers estejam funcionando conforme o esperado dentro dos pods. Ele assume o controle de cada n√≥, garantindo que os containers estejam sendo executados conforme as instru√ß√µes recebidas do `control plane`. Ele monitora constantemente o estado atual dos `pods` e compara-os com o estado desejado. Caso haja alguma diverg√™ncia, o `kubelet` faz os ajustes necess√°rios para que os containers sigam funcionando perfeitamente.

* O `kube-proxy`, que √© o componente respons√°vel fazer ser poss√≠vel que os `pods` e os `services` se comuniquem entre si e com o mundo externo. Ele observa o `control plane` para identificar mudan√ßas na configura√ß√£o dos servi√ßos e, em seguida, atualiza as regras de encaminhamento de tr√°fego para garantir que tudo continue fluindo conforme o esperado.


* Todos os `pods` de nossas aplica√ß√µes.



#### Formas de instalar o Kubernetes

Hoje n√≥s iremos focar a instala√ß√£o do Kubernetes utilizando o `kubeadm`, que √© uma das formas mais antigas para a cria√ß√£o de um cluster Kubernetes. Mas existem outras formas de instalar o Kubernetes, vou detalhar algumas delas aqui:

* **`kubeadm`**: √â uma ferramenta para criar e gerenciar um cluster Kubernetes em v√°rios n√≥s. Ele automatiza muitas das tarefas de configura√ß√£o do cluster, incluindo a instala√ß√£o do control plane e dos nodes. √â altamente configur√°vel e pode ser usado para criar clusters personalizados.

* **`Kubespray`**: √â uma ferramenta que usa o Ansible para implantar e gerenciar um cluster Kubernetes em v√°rios n√≥s. Ele oferece muitas op√ß√µes para personalizar a instala√ß√£o do cluster, incluindo a escolha do provedor de rede, o n√∫mero de r√©plicas do control plane, o tipo de armazenamento e muito mais. √â uma boa op√ß√£o para implantar um cluster em v√°rios ambientes, incluindo nuvens p√∫blicas e privadas.

* **`Cloud Providers`**: Muitos provedores de nuvem, como AWS, Google Cloud Platform e Microsoft Azure, oferecem op√ß√µes para implantar um cluster Kubernetes em sua infraestrutura. Eles geralmente fornecem modelos predefinidos que podem ser usados para implantar um cluster com apenas alguns cliques. Alguns provedores de nuvem tamb√©m oferecem servi√ßos gerenciados de Kubernetes que lidam com toda a configura√ß√£o e gerenciamento do cluster.

* **`Kubernetes Gerenciados`**: S√£o servi√ßos gerenciados oferecidos por alguns provedores de nuvem, como Amazon EKS, o GKE do Google Cloud e o AKS, da Azure. Eles oferecem um cluster Kubernetes gerenciado onde voc√™ s√≥ precisa se preocupar em implantar e gerenciar seus aplicativos. Esses servi√ßos lidam com a configura√ß√£o, atualiza√ß√£o e manuten√ß√£o do cluster para voc√™. Nesse caso, voc√™ n√£o tem que gerenciar o `control plane` do cluster, pois ele √© gerenciado pelo provedor de nuvem.

* **`Kops`**: √â uma ferramenta para implantar e gerenciar clusters Kubernetes na nuvem. Ele foi projetado especificamente para implanta√ß√£o em nuvens p√∫blicas como AWS, GCP e Azure. Kops permite criar, atualizar e gerenciar clusters Kubernetes na nuvem. Algumas das principais vantagens do uso do Kops s√£o a personaliza√ß√£o, escalabilidade e seguran√ßa. No entanto, o uso do Kops pode ser mais complexo do que outras op√ß√µes de instala√ß√£o do Kubernetes, especialmente se voc√™ n√£o estiver familiarizado com a nuvem em que est√° implantando.

* **`Minikube` e `kind`**: S√£o ferramentas que permitem criar um cluster Kubernetes localmente, em um √∫nico n√≥. S√£o √∫teis para testar e aprender sobre o Kubernetes, pois voc√™ pode criar um cluster em poucos minutos e come√ßar a implantar aplicativos imediatamente. Elas tamb√©m s√£o √∫teis para pessoas desenvolvedoras que precisam testar suas aplica√ß√µes em um ambiente Kubernetes sem precisar configurar um cluster em um ambiente de produ√ß√£o.

Ainda existem outras formas de instalar o Kubernetes, mas essas s√£o as mais comuns. Para mais detalhes sobre as outras formas de instalar o Kubernetes, voc√™ pode consultar a documenta√ß√£o oficial do Kubernetes.


#### Criando um cluster Kubernetes com o kubeadm

Agora que voc√™ j√° sabe o que √© o Kubernetes e quais s√£o as suas principais funcionalidades, vamos come√ßar a instalar o Kubernetes em nosso cluster. Nesse momento iremos ver como realizar a cria√ß√£o de um cluster Kubernetes utilizando o `kubeadm`, por√©m no decorrer da nossa jornada iremos ver outras formas de instalar o Kubernetes.

Como falado, o `kubeadm` √© uma ferramenta para criar e gerenciar um cluster Kubernetes em v√°rios n√≥s. Ele automatiza muitas das tarefas de configura√ß√£o do cluster, incluindo a instala√ß√£o do `control plane` e dos `nodes`.

Primeira coisa, para que possamos seguir em frente, temos que entender quais s√£o os pr√©-requisitos para a instala√ß√£o do Kubernetes. Para isso, voc√™ pode consultar a documenta√ß√£o oficial do Kubernetes, mas vou listar aqui os principais pr√©-requisitos:

* Linux

* 2 GB ou mais de RAM por m√°quina (menos de 2 GB n√£o √© recomendado)

* 2 CPUs ou mais

* Conex√£o de rede entre todas os nodes no cluster (pode ser via rede p√∫blica ou privada)

* Algumas portas precisam estar abertas para que o cluster funcione corretamente, as principais:

    * Porta 6443: √â a porta padr√£o usada pelo Kubernetes API Server para se comunicar com os componentes do cluster. √â a porta principal usada para gerenciar o cluster e deve estar sempre aberta.

    * Portas 10250-10255: Essas portas s√£o usadas pelo kubelet para se comunicar com o control plane do Kubernetes. A porta 10250 √© usada para comunica√ß√£o de leitura/grava√ß√£o e a porta 10255 √© usada apenas para comunica√ß√£o de leitura.

    * Porta 30000-32767: Essas portas s√£o usadas para servi√ßos NodePort que precisam ser acess√≠veis fora do cluster. O Kubernetes aloca uma porta aleat√≥ria dentro desse intervalo para cada servi√ßo NodePort e redireciona o tr√°fego para o pod correspondente.

    * Porta 2379-2380: Essas portas s√£o usadas pelo etcd, o banco de dados de chave-valor distribu√≠do usado pelo control plane do Kubernetes. A porta 2379 √© usada para comunica√ß√£o de leitura/grava√ß√£o e a porta 2380 √© usada apenas para comunica√ß√£o de elei√ß√£o.


&nbsp;

##### Instalando o kubeadm
Estamos aqui para configurar nosso ambiente Kubernetes e, olha s√≥, √© simples como voar! Vamos l√°!

##### Desativando o uso do swap no sistema

Primeiro, vamos desativar a utiliza√ß√£o de swap no sistema. Isso √© necess√°rio porque o Kubernetes n√£o trabalha bem com swap ativado:

```
sudo swapoff -a
```

##### Carregando os m√≥dulos do kernel

Agora, vamos carregar os m√≥dulos do kernel necess√°rios para o funcionamento do Kubernetes:

```
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter
```

##### Configurando par√¢metros do sistema

Em seguida, vamos configurar alguns par√¢metros do sistema. Isso garantir√° que nosso cluster funcione corretamente:

```
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system
```

##### Instalando os pacotes do Kubernetes

Hora de instalar os pacotes do Kubernetes! Coisa linda de ai meu Deus! Aqui vamos n√≥s:

```
sudo apt-get update && sudo apt-get install -y apt-transport-https curl

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl

sudo apt-mark hold kubelet kubeadm kubectl
```

##### Instalando o containerd

Em seguida, vamos instalar o containerd, que s√£o essenciais para nosso ambiente Kubernetes:

```
sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt-get update && sudo apt-get install -y containerd.io
```

##### Configurando o containerd

Agora, vamos configurar o containerd para que ele funcione adequadamente com o nosso cluster:

```
sudo containerd config default | sudo tee /etc/containerd/config.toml

sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl status containerd
```

##### Habilitando o servi√ßo do kubelet

Por fim, vamos habilitar o servi√ßo do kubelet para que ele inicie automaticamente com o sistema:

```
sudo systemctl enable --now kubelet
```

##### Configurando as portas

Antes de iniciar o cluster, lembre-se das portas que precisam estar abertas para que o cluster funcione corretamente. Precisamos ter as portas TCP 6443, 10250-10255, 30000-32767 e 2379-2380 abertas entre os nodes do cluster. No nosso exemplo, onde estaremos usando somente um node `control plane`, n√£o precisamos nos preocupar com algumas quando temos mais de um node `control plane`, pois eles precisam se comunicar entre si para manter o estado do cluster, ou ainda as portas 30000-32767, que s√£o usadas para servi√ßos NodePort que precisam ser acess√≠veis fora do cluster. Elas n√≥s podemos ir abrindo conforme a necessidade, conforme vamos criando os nossos servi√ßos.

Por agora o que precisamos garantir s√£o as portas TCP 6443 somente no `control plane` e as 10250-10255 abertas em todos nodes do cluster.

Em nosso exemplo vamos utilizar como CNI o Weave Net, que √© um CNI que utiliza o protocolo de roteamento de pacotes do Kubernetes para criar uma rede entre os pods. Falarei mais sobre ele mais pra frente, mas como estamos falando das portas que s√£o importantes para o cluster funcionar, precisamos abrir a porta TCP 6783 e as portas UDP 6783 e 6784, para que o Weave Net funcione corretamente.

Ent√£o j√° sabe, n√£o esque√ßa de abrir as portas TCP 6443, 10250-10255 e 6783 no seu firewall.

##### Inicializando o cluster

Agora que temos tudo configurado, vamos iniciar o nosso cluster:

```
sudo kubeadm init --pod-network-cidr=10.10.0.0/16 --apiserver-advertise-address=<O IP QUE VAI FALAR COM OS NODES>
```

&nbsp;


Substitua `<O IP QUE VAI FALAR COM OS NODES>` pelo endere√ßo IP da m√°quina que est√° atuando como `control plane`.

Ap√≥s a execu√ß√£o bem-sucedida do comando acima, voc√™ ver√° uma mensagem informando que o cluster foi inicializado com sucesso e todos os detalhes de sua inicializa√ß√£o, conforme √© poss√≠vel ver na sa√≠da do comando:

```bash
[init] Using Kubernetes version: v1.26.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.31.57.89]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-01 localhost] and IPs [172.31.57.89 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-01 localhost] and IPs [172.31.57.89 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 7.504091 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: if9hn9.xhxo6s89byj9rsmd
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.31.57.89:6443 --token if9hn9.xhxo6s89byj9rsmd \
	--discovery-token-ca-cert-hash sha256:ad583497a4171d1fc7d21e2ca2ea7b32bdc8450a1a4ca4cfa2022748a99fa477 

```

&nbsp;

Inclusive, voc√™ ver√° uma lista de comandos para configurar o acesso ao cluster com o kubectl. Copie e cole esse comando em seu terminal:

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

&nbsp;

Essa configura√ß√£o √© necess√°ria para que o kubectl possa se comunicar com o cluster, pois quando estamos copiando o arquivo `admin.conf` para o diret√≥rio `.kube` do usu√°rio, estamos copiando o arquivo com as permiss√µes de root, esse √© o motivo de executarmos o comando `sudo chown $(id -u):$(id -g) $HOME/.kube/config` para alterar as permiss√µes do arquivo para o usu√°rio que est√° executando o comando.


##### Entendendo o arquivo admin.conf
Agora precisamos entender o que temos dentro do arquivo `admin.conf`. Antes de mais nada precisamos conhecer alguns pontos importantes sobre o a estrutura do arquivo `admin.conf`:

- √â um arquivo de configura√ß√£o do kubectl, que √© o cliente de linha de comando do Kubernetes. Ele √© usado para se comunicar com o cluster Kubernetes.

- Cont√©m as informa√ß√µes de acesso ao cluster, como o endere√ßo do servidor API, o certificado de cliente e o token de autentica√ß√£o.

- Eu posso ter mais de um contexto dentro do arquivo `admin.conf`, onde cada contexto √© um cluster Kubernetes. Por exemplo, eu posso ter um contexto para o cluster de produ√ß√£o e outro para o cluster de desenvolvimento, simples como voar.

- Ele cont√©m os dados de acesso ao cluster, portanto, se algu√©m tiver acesso a esse arquivo, ele ter√° acesso ao cluster. (Desde que tenha acesso ao cluster, claro).

- O arquivo `admin.conf` √© criado quando o cluster √© inicializado.


Vou copiar aqui o conte√∫do de um exemplo de arquivo `admin.conf`:

```yaml
apiVersion: v1

clusters:
- cluster:
    certificate-authority-data: SEU_CERTIFICADO_AQUI
    server: https://172.31.57.89:6443
  name: kubernetes

contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes

current-context: kubernetes-admin@kubernetes

kind: Config

preferences: {}

users:
- name: kubernetes-admin
  user:
    client-certificate-data: SUA_CHAVE_PUBLICA_AQUI
    client-key-data: SUA_CHAVE_PRIVADA_AQUI
```

&nbsp;

Simplificando, temos a seguinte estrutura:

```yaml
apiVersion: v1
clusters:
#...
contexts:
#...
current-context: kind-kind-multinodes
kind: Config
preferences: {}
users:
#...
```

&nbsp;

Vamos ver o que temos dentro de cada se√ß√£o:

**Clusters**

A se√ß√£o clusters cont√©m informa√ß√µes sobre os clusters Kubernetes que voc√™ deseja acessar, como o endere√ßo do servidor API e o certificado de autoridade. Neste arquivo, h√° somente um cluster chamado kubernetes, que √© o cluster que acabamos de criar.

```yaml
- cluster:
    certificate-authority-data: SEU_CERTIFICADO_AQUI
    server: https://172.31.57.89:6443
  name: kubernetes
```

&nbsp;

**Contextos**

A se√ß√£o contexts define configura√ß√µes espec√≠ficas para cada combina√ß√£o de cluster, usu√°rio e namespace. N√≥s somente temos um contexto configurado. Ele √© chamado kubernetes-admin@kubernetes e combina o cluster kubernetes com o usu√°rio kubernetes-admin.

```yaml
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
```

&nbsp;


**Contexto atual**

A propriedade current-context indica o contexto atualmente ativo, ou seja, qual combina√ß√£o de cluster, usu√°rio e namespace ser√° usada ao executar comandos kubectl. Neste arquivo, o contexto atual √© o kubernetes-admin@kubernetes.

```yaml
current-context: kubernetes-admin@kubernetes
```

&nbsp;

**Prefer√™ncias**

A se√ß√£o preferences cont√©m configura√ß√µes globais que afetam o comportamento do kubectl. Aqui podemos definir o editor de texto padr√£o, por exemplo.

```yaml
preferences: {}
```

&nbsp;

**Usu√°rios**

A se√ß√£o users cont√©m informa√ß√µes sobre os usu√°rios e suas credenciais para acessar os clusters. Neste arquivo, h√° somente um usu√°rio chamado kubernetes-admin. Ele cont√©m os dados do certificado de cliente e da chave do cliente.

```yaml
- name: kubernetes-admin
  user:
    client-certificate-data: SUA_CHAVE_PUBLICA_AQUI
    client-key-data: SUA_CHAVE_PRIVADA_AQUI
```

&nbsp;


Outra informa√ß√£o super importante que est√° contida nesse arquivo √© referente as credenciais de acesso ao cluster. Essas credenciais s√£o usadas para autenticar o usu√°rio que est√° executando o comando kubectl. Essas credenciais s√£o:

- **Token de autentica√ß√£o**: √â um token de acesso que √© usado para autenticar o usu√°rio que est√° executando o comando kubectl. Esse token √© gerado automaticamente quando o cluster √© inicializado. Esse token √© usado para autenticar o usu√°rio que est√° executando o comando kubectl. Esse token √© gerado automaticamente quando o cluster √© inicializado.

- **certificate-authority-data**: Este campo cont√©m a representa√ß√£o em base64 do certificado da autoridade de certifica√ß√£o (CA) do cluster. A CA √© respons√°vel por assinar e emitir certificados para o cluster. O certificado da CA √© usado para verificar a autenticidade dos certificados apresentados pelo servidor de API e pelos clientes, garantindo que a comunica√ß√£o entre eles seja segura e confi√°vel.

- **client-certificate-data**: Este campo cont√©m a representa√ß√£o em base64 do certificado do cliente. O certificado do cliente √© usado para autenticar o usu√°rio ao se comunicar com o servidor de API do Kubernetes. O certificado √© assinado pela autoridade de certifica√ß√£o (CA) do cluster e inclui informa√ß√µes sobre o usu√°rio e sua chave p√∫blica.

- **client-key-data**: Este campo cont√©m a representa√ß√£o em base64 da chave privada do cliente. A chave privada √© usada para assinar as solicita√ß√µes enviadas ao servidor de API do Kubernetes, permitindo que o servidor verifique a autenticidade da solicita√ß√£o. A chave privada deve ser mantida em sigilo e n√£o compartilhada com outras pessoas ou sistemas.

Esses campos s√£o importantes para estabelecer uma comunica√ß√£o segura e autenticada entre o cliente (geralmente o kubectl ou outras ferramentas de gerenciamento) e o servidor de API do Kubernetes. Eles permitem que o servidor de API verifique a identidade do cliente e vice-versa, garantindo que apenas usu√°rios e sistemas autorizados possam acessar e gerenciar os recursos do cluster.

&nbsp;

Voc√™ pode encontrar os arquivos que s√£o utilizados para adicionar essas credentiais ao seu cluster em `/etc/kubernetes/pki/`.
L√° temos os seguintes arquivos que s√£o utilizados para adicionar essas credenciais ao seu cluster:

- **client-certificate-data**: O arquivo de certificado do cliente geralmente √© encontrado em /etc/kubernetes/pki/apiserver-kubelet-client.crt.

- **client-key-data**: O arquivo da chave privada do cliente geralmente √© encontrado em /etc/kubernetes/pki/apiserver-kubelet-client.key.

- **certificate-authority-data**: O arquivo do certificado da autoridade de certifica√ß√£o (CA) geralmente √© encontrado em /etc/kubernetes/pki/ca.crt.

Vale lembrar que esse arquivo √© gerado automaticamente quando o cluster √© inicializado, e s√£o adicionados ao arquivo `admin.conf` que √© utilizado para acessar o cluster. Essas credenciais s√£o copiadas para o arquivo `admin.conf` j√° convertidas para base64.

&nbsp;

Pronto, agora voc√™ j√° sabe o porqu√™ copiamos o arquivo `admin.conf` para o diret√≥rio `~/.kube/` e como ele funciona.


Caso voc√™ queira, voc√™ pode acessar o conte√∫do do arquivo `admin.conf` com o seguinte comando:


```bash
kubectl config view
```

&nbsp;

Ele somente ir√° omitir os dados de certificados e chaves privadas, que s√£o muito grandes para serem exibidos no terminal.

&nbsp;

##### Adicionando os demais nodes ao cluster

Agora que j√° temos o nosso cluster inicializado e j√° estamos entendendo muito bem o que √© o arquivo `admin.conf`, chegou o momento de adicionar os demais nodes ao nosso cluster.

Para isso, vamos novamente utilizar o comando `kubeadm`, por√©m ao inv√©s de executar o comando no node do control plane, nesse momento precisamos rodar o comando diretamente no node que queremos adicionar ao cluster.

Quando inicializamos o nosso cluster, o `kubeadm` nos mostrou o comando que precisamos executar no novos nodes, para que eles possam ser adicinados ao cluster como `workers`.

```bash
sudo kubeadm join 172.31.57.89:6443 --token if9hn9.xhxo6s89byj9rsmd \
	--discovery-token-ca-cert-hash sha256:ad583497a4171d1fc7d21e2ca2ea7b32bdc8450a1a4ca4cfa2022748a99fa477 
```

&nbsp;

O comando kubeadm join √© usado para adicionar um novo node ao cluster Kubernetes existente. Ele √© executado nos worker nodes para que eles possam se juntar ao cluster e receber instru√ß√µes do control plane. Vamos analisar as partes do comando fornecido:

- **kubeadm join**: O comando base para adicionar um novo n√≥ ao cluster.

- **172.31.57.89:6443**: Endere√ßo IP e porta do servidor de API do n√≥ mestre (control plane). Neste exemplo, o n√≥ mestre est√° localizado no IP 172.31.57.89 e a porta √© 6443.

- **--token if9hn9.xhxo6s89byj9rsmd**: O token √© usado para autenticar o n√≥ trabalhador no n√≥ mestre durante o processo de ades√£o. Os tokens s√£o gerados pelo n√≥ mestre e t√™m uma validade limitada (por padr√£o, 24 horas). Neste exemplo, o token √© if9hn9.xhxo6s89byj9rsmd.

- **--discovery-token-ca-cert-hash sha256:ad583497a4171d1fc7d21e2ca2ea7b32bdc8450a1a4ca4cfa2022748a99fa477**: Este √© um hash criptogr√°fico do certificado da autoridade de certifica√ß√£o (CA) do control plane. Ele √© usado para garantir que o n√≥ worker esteja se comunicando com o n√≥ do control plane correto e aut√™ntico. O valor ap√≥s sha256: √© o hash do certificado CA.

Ao executar este comando no worker, ele iniciar√° o processo de ades√£o ao cluster. Se o token for v√°lido e o hash do certificado CA corresponder ao certificado CA do n√≥ do control plane, o n√≥ worker ser√° autenticado e adicionado ao cluster. Ap√≥s a ades√£o bem-sucedida, o novo node come√ßar√° a executar os Pods e a receber instru√ß√µes do control plane, conforme necess√°rio.

Ap√≥s executar o `join` em cada worker node, v√° at√© o node que criamos para ser o control plane, e execute:

```bash
kubectl get nodes
```

&nbsp;

```bash
NAME     STATUS   ROLES           AGE   VERSION
k8s-01   NotReady    control-plane   4m   v1.26.3
k8s-02   NotReady    <none>          3m   v1.26.3
k8s-03   NotReady    <none>          3m   v1.26.3
```

&nbsp;

Agora voc√™ j√° consegue ver que os dois novos nodes foram adicionados ao cluster, por√©m ainda est√£o com o status `Not Ready`, pois ainda n√£o instalamos o nosso plugin de rede para que seja poss√≠vel a comunica√ß√£o entre os pods. Vamos resolver isso agora. :)

##### Instalando o Weave Net

Agora que o cluster est√° inicializado, vamos instalar o Weave Net:

```
$ kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
```

&nbsp;

Aguarde alguns minutos at√© que todos os componentes do cluster estejam em funcionamento. Voc√™ pode verificar o status dos componentes do cluster com o seguinte comando:

```
kubectl get pods -n kube-system
```
&nbsp;

```
kubectl get nodes
```

&nbsp;

```bash
NAME     STATUS   ROLES           AGE   VERSION
k8s-01   Ready    control-plane   7m   v1.26.3
k8s-02   Ready    <none>          6m   v1.26.3
k8s-03   Ready    <none>          6m   v1.26.3
```

&nbsp;

O Weave Net √© um plugin de rede que permite que os pods se comuniquem entre si. Ele tamb√©m permite que os pods se comuniquem com o mundo externo, como outros clusters ou a Internet. 
Quando o Kubernetes √© instalado, ele resolve v√°rios problemas por si s√≥, por√©m quando o assunto √© a comunica√ß√£o entre os pods, ele n√£o resolve. Por isso, precisamos instalar um plugin de rede para resolver esse problema.

##### O que √© o CNI?

CNI √© uma especifica√ß√£o e conjunto de bibliotecas para a configura√ß√£o de interfaces de rede em containers. A CNI permite que diferentes solu√ß√µes de rede sejam integradas ao Kubernetes, facilitando a comunica√ß√£o entre os Pods (grupos de containers) e servi√ßos.

Com isso, temos diferentes plugins de redes, que seguem a especifica√ß√£o CNI, e que podem ser utilizados no Kubernetes. O Weave Net √© um desses plugins de rede.

Entre os plugins de rede mais utilizados no Kubernetes, temos:

- **Calico** √© um dos plugins de rede mais populares e amplamente utilizados no Kubernetes. Ele fornece seguran√ßa de rede e permite a implementa√ß√£o de pol√≠ticas de rede. O Calico utiliza o BGP (Border Gateway Protocol) para rotear tr√°fego entre os n√≥s do cluster, proporcionando um desempenho eficiente e escal√°vel.

- **Flannel** √© um plugin de rede simples e f√°cil de configurar, projetado para o Kubernetes. Ele cria uma rede overlay que permite que os Pods se comuniquem entre si, mesmo em diferentes n√≥s do cluster. O Flannel atribui um intervalo de IPs a cada n√≥ e utiliza um protocolo simples para rotear o tr√°fego entre os n√≥s.

- **Weave** √© outra solu√ß√£o popular de rede para Kubernetes. Ele fornece uma rede overlay que permite a comunica√ß√£o entre os Pods em diferentes n√≥s. Al√©m disso, o Weave suporta criptografia de rede e gerenciamento de pol√≠ticas de rede. Ele tamb√©m pode ser integrado com outras solu√ß√µes, como o Calico, para fornecer recursos adicionais de seguran√ßa e pol√≠ticas de rede.

- **Cilium** √© um plugin de rede focado em seguran√ßa e desempenho. Ele utiliza o BPF (Berkeley Packet Filter) para fornecer pol√≠ticas de rede e seguran√ßa de alto desempenho. O Cilium tamb√©m oferece recursos avan√ßados, como balanceamento de carga, monitoramento e solu√ß√£o de problemas de rede.

- **Kube-router** √© uma solu√ß√£o de rede leve para Kubernetes. Ele utiliza o BGP e IPVS (IP Virtual Server) para rotear o tr√°fego entre os n√≥s do cluster, proporcionando um desempenho eficiente e escal√°vel. Kube-router tamb√©m suporta pol√≠ticas de rede e permite a implementa√ß√£o de firewalls entre os Pods.

Esses s√£o apenas alguns dos plugins de rede mais populares e amplamente utilizados no Kubernetes. Voc√™ pode encontrar uma lista completa de plugins de rede no site do Kubernetes.

Agora, qual voc√™ dever√° escolher? A resposta √© simples: o que melhor se adequar √†s suas necessidades. Cada plugin de rede tem suas vantagens e desvantagens, e voc√™ deve escolher aquele que melhor se adequar ao seu ambiente.

Minha dica, procure n√£o ficar inventando muita moda, tenta utilizar os que s√£o j√° validados e bem aceitos pela comunidade, como o Weave Net, Calico, Flannel, etc. 

O meu preferido √© o `Weave Net` pela simplicidade de instala√ß√£o e os recursos oferecidos.

Um cara que eu tenho gostado bastante √© o `Cilium`, ele √© bem completo e tem uma comunidade bem ativa, al√©m de utilizar o BPF, que √© um assunto super quente no mundo Kubernetes!

&nbsp;

Pronto, j√° temos o nosso cluster inicializado e o Weave Net instalado. Agora, vamos criar um Deployment para testar a comunica√ß√£o entre os Pods.

```bash
kubectl create deployment nginx --image=nginx --replicas 3
```

&nbsp;

```bash
kubectl get pods -o wide
```

&nbsp;

```bash
NAME                     READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
nginx-748c667d99-8brrj   1/1     Running   0          12s   10.32.0.4   k8s-02   <none>           <none>
nginx-748c667d99-8knx2   1/1     Running   0          12s   10.40.0.2   k8s-03   <none>           <none>
nginx-748c667d99-l6w7r   1/1     Running   0          12s   10.40.0.1   k8s-03   <none>           <none>
```

&nbsp;

Pronto, nosso cluster est√° funcionando e os Pods est√£o em execu√ß√£o em diferentes n√≥s.

Agora voc√™ j√° pode se divertir e utilizar o seu mais novo cluster Kubernetes!

&nbsp;
